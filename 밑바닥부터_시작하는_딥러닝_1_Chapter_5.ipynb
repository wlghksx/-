{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "밑바닥부터 시작하는 딥러닝 1 Chapter 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXxmR47UzMGwdre4E/1dCm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wlghksx/Deep-Learning-from-Scratch/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_1_Chapter_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5-R8ohjLalu"
      },
      "source": [
        "# Chapter 5 오차역전파법(Backpropagation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDiIa7MhQX36"
      },
      "source": [
        "앞 장에서는 신경망 학습에 대해서 설명. 그때 신경망의 가중치 매개변수의 기울기는 수치 미분을 사용하여 구했다. 수치 미분은 단순하고 구현하기도 쉽지만 시간이 오래걸린다는게 단점이다. 이번 장에서는 가중치 매개변수의 기울기를 효율적으로 계산하는 '오차역전파법(Backpropagation)'을 배워보겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF9RHTA3RYIO"
      },
      "source": [
        "이 책에서는 수식이 아닌 좀 더 직관적인 계싼 그래프를 통해 시각적으로 이해해보도록 하자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt6oVMz8SilT"
      },
      "source": [
        "## 5.1 계산 그래프"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vin5GTlhSxKs"
      },
      "source": [
        ": 계산 과정을 그래프로 나타낸 것. 노드와 엣지를 가지고 표현."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRMN9rSqS7zx"
      },
      "source": [
        ": 계산을 왼쪽에서 오른쪽으로 진행하는 것을 순전파 (Forward propagation)\n",
        "\n",
        ": 반대로 진행하는 것을 역전파(Back propagation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYr9_wG4ThH1"
      },
      "source": [
        "## 5.1.2 국소적 계산"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpJifg1xU8QH"
      },
      "source": [
        ": 국소적 계산이란 '자신과 직접 관계된 작은 범위' 라는 뜻.\n",
        "\n",
        ": 결국 계산 그래프는 전체 계산이 아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다. 따라서 계산은 단순하지만, 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKrQ6u6VYPSF"
      },
      "source": [
        "## 5.1.3 왜 계산 그래프로 푸는가?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IruCuCcYdnN"
      },
      "source": [
        "실제 계산 그래프를 사용하는 가장 큰 이유는 역전파를 통해 '미분'을 효울적으로 계산할 수 있는 점에 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQMmovOkYrHD"
      },
      "source": [
        "계산그래프에서의 거꾸로 화살표가 결국 미분값에 해당한다. 즉 변수의 변화에 따라 결과가 얼마나 달라지는지를 나타내주는 값이 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiJaAq6NbFpd"
      },
      "source": [
        "## 5.2 연쇄법칙"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wmsSoLcbLrE"
      },
      "source": [
        "즉, 후의 노드의 신호에 국소적 미분을 곱한 값을 이전 노드로 다시 전달하는 것을 역전파라 하는데, 이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다.\n",
        "\n",
        "이러한 역전파를 연쇄법칙의 원리로 설명할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NICPCnrMb7EF"
      },
      "source": [
        "## 5.3 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pCbUfVbcYn8"
      },
      "source": [
        "## 5.3.1 덧셈 노드의 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKyxgGhccn4-"
      },
      "source": [
        "덧셈 노드에서의 역전파는 결국 결과적으로 입력된 값을 그대로 다음노드로 보내게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02XcxRfbc91O"
      },
      "source": [
        "## 5.3.2 곱셈 노드의 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JeZJN5wdKlc"
      },
      "source": [
        "곱셈 노드에서의 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값을'곱하여 하류로 보낸다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8hzm8nhdnoc"
      },
      "source": [
        "# 5.4 단순한 계층 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3F8leYldy5M"
      },
      "source": [
        "## 5.4.1 곱셈 계층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zQonqUXeBjN"
      },
      "source": [
        "class MulLayer :\n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  def forward(self,x,y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x*y\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def backward(self,dout):\n",
        "    dx = dout * self.y # x와 y를 바꾼다\n",
        "    dy = dout * self.x\n",
        "\n",
        "    return dx, dy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9a9du4trzLq"
      },
      "source": [
        "# coding: utf-8\n",
        "from layer_naive import *\n",
        "\n",
        "\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(\"price:\", int(price))\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dTax:\", dtax)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMrP8b3lsiGZ"
      },
      "source": [
        "# 5.5 활성화 함수 계층 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDJGzSNas75a"
      },
      "source": [
        "## 5.5.1 ReLU 계층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH4aHfnbs-ci"
      },
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hatvGshSthTF"
      },
      "source": [
        "순전파 때의 입력 값이 0 이하면 역전파 때의 값은 0이 돼야 한다. 그래서 역전파 때는 순전파 때 만들어 둔 mask를 써서 mask 원소가 True인 곳에는 상류에서 전파된 dout를 0으로 설정한다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92urc5AZtwg7"
      },
      "source": [
        "## 5.5.2 Sigmoid 계층"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8acRGWbWt1CR"
      },
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1AK07cguIRX"
      },
      "source": [
        "# 5.6 Affine/Softmax 계층 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgNqrGgYu2pC"
      },
      "source": [
        "## 5.6.1 Affine 계층"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x74ukPpQu6T9"
      },
      "source": [
        "신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서는 '어파인 변환'이라고 한다. 그래서 이 책에서는 어파인 변환을 수행하는 처리를 'Affine 계층'이라는 이름으로 구현한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXd625s97QRK"
      },
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        \n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        \n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebiJVD7C72Xd"
      },
      "source": [
        "## 5.6.1 Softmax - with - Loss 계층"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF1Bt16H8Xvq"
      },
      "source": [
        "마지막으로 출력층에서 사용하는 소프트맥스 함수에 관해 설명하겠다.\n",
        "\n",
        "앞에서 말했듯이 소프트맥스 함수는 입력 값을 정규화(출력의 합이 1이 되도록)하여 출력한다.\n",
        "\n",
        "손글씨 숫자는 가짓수가 10개(10클래스 분류)이므로 Softmax계층의 입력은 10개가 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v4MB5z-8a1y"
      },
      "source": [
        "신경망에서 주로 수행하는 작업은 학습과 추론이다. 추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다. 예컨대 신경망은 추론할 떄는 마지막 Affine 계층의 출력을 인식 결과로 이용한다. 또한 신경망에서 정규화하지 않는 출력 결과에서는 Softmax 앞의 Affine 계를 점수라고 한다. \n",
        "\n",
        "즉, 신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되나 Softmax 계층은 필요 없다는 것이다. \n",
        "\n",
        "하지만 신경망을 학습할 때는 Softmax 계층이 필요하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSfGev0P9aGz"
      },
      "source": [
        "이제 소프트맥스 계츠을 구현할 텐데, 교차 엔트로피 오차도 포함하여 'Softmax - with - Loss 계층'이라는 이름으로 구현한다. 먼저 Softmax- with-Loss 계층의 계산 그래프를 살펴보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHw4d5Po9uDv"
      },
      "source": [
        "소프트맥스의 손실함수로 '교차 엔트로피 오차'를 사용하면 softmax 계층의 역전파가 (y1-t1, y2-t2, y3-t3) 로 말끔히 떨어진다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VUBYoz4_6D8"
      },
      "source": [
        "또한 회귀의 출력층에서 사용하는 항등함수의 손실함수로 오차제곱합을 사용하는 이유도 이와 같다. 즉, 항등함수의 손실함수로 오차제곱합을 사용하면 역전파의 결과가 (y1-t1,y2-t2,y3-t3)로 말끔히 떨어진다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iLSSZl9AGV0"
      },
      "source": [
        "예를 들어, 정답 레이블이 (0,1,0) 일 떄, Softmax 계층이 (0.3,0.2,0.5)를 출력했다고 한다면,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vTM8_sOARLW"
      },
      "source": [
        "정답의 인덱스는 1인데 이때의 확률은 겨우 20%라서 이 시점의 신경망은 제대로 인식하지 못하고 있는 것이다. 따라서 결과적으로 역전파는 (0.3, -0.8, 0.5)라는 커다란 오차를 전파하게 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c03PEEkkAlCu"
      },
      "source": [
        "결과적으로 Softmax 앞 계층들은 그 큰 오차로부터 큰 깨달음을 얻게 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obPnlEBVApfZ"
      },
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        \n",
        "        return dx\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Finr0PokAtVL"
      },
      "source": [
        "# 5.7 오차역전파법 구하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84DBKpAYBEEf"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhnjwFvmERXP"
      },
      "source": [
        "## 5.7.3 오차역전파법으로 구한 기울기 검증하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrJPgrh7F4z8"
      },
      "source": [
        "결국 지금까지 기울기를 구하는 방법을 2가지를 설명했다.\n",
        "1. 수치 미분을 써서 구하는 방법\n",
        "2. 해석적으로 수식을 풀어 구하는 방법\n",
        "\n",
        "=> 후자인 해석적 방법은 오차역전파법을 이용하여 매개변수가 많아도 효율적으로 계산할 수 있었다. 그러니 이제부터는 느린 수치 미분 대신 오차역전파법을 사용하도록 하자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9XO194rGL4l"
      },
      "source": [
        "- 하지만 수치미분이 아무른 필요가 없는 것은 아니다. 수치 미분의 이점은 구현하기가 쉽다는 점이다. 수치미분의 구현에는 버그가 숨어있기 어려운 반면, 오차역전파법은 복잡해서 종종 실수를 하곤 한다. 그래서 수치 미분의 결과와 오차역전파법의 결과를 비교하여 제대로 구현했는지 검증할 수 있다.\n",
        "\n",
        "- 이렇게 두 방식으로 구한 기울기가 동일함을 확인하는 작업을 '기울기 확인'이라고 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hto9B2GmGoB4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}